ùï∂ùñîùñóùñáùñéùñôùñàùñç
# Przyk≈Çady u≈ºycia
## Sprawdzanie parzysto≈õci
### Dane wej≈õciowe
$\vec{u}$ - $n$-elementowy wektor sk≈ÇadajƒÖcy siƒô z 0 i 1
### Dane wyj≈õciowe
$1$, je≈õli liczba jedynek jest nieparzysta,\
$0$ w przeciwnym przypadku
### Struktura sieci
Dwuwarstwowa sieƒá jednokierunkowa z $n$ wej≈õƒá i jednym wyj≈õciu
### Zbi√≥r danych uczƒÖcych
Udowodniono, ≈ºe dla wektora o rozmiarze $n$ sk≈ÇadajƒÖcego siƒô z $0$ i $1$ wystarczy zbi√≥r danych
uczƒÖcych o wielko≈õci $2^n$.

$
{u,z} = \begin{bmatrix}
1 & 0 &0&0&|1\\
1&1&0&0&|0\\
0&1&1&1&|1\\
\vdots&\vdots&\vdots&\vdots&\vdots
\end{bmatrix}
$

## Kompresja
Zadanie kompresji danych polega na zmniejszeniu ilo≈õci informacji przechowywanych lub
transmitowanych przy zachowaniu mo≈ºliwo≈õci jej pe≈Çnego odtworzenia (dekompresji). Mamy kompresjƒô
stratnƒÖ i bez stratnƒÖ. Przyk≈Çadem kompresji bez stratnej mo≈ºe byƒá drzewo Huffmana
### Dane wej≈õciowe
$n$-elementowy wektor $\vec{u}$
### Dane wyj≈õciowe
$n$-elementowy wektor $\vec{y}$
### Struktura sieci
Tr√≥jwarstwowa sieƒá jednokierunkowa

$i=1,2,...,n$ - Indeksy wej≈õciowe\
$j=1,2,...,n$ - Indeksy wyj≈õciowe\
$k=1,2,...,q; q \leq n$ - Indeksy warstwy ukrytej

### Zestaw uczƒÖcy
$\{\vec{u}^\mu,\vec{u}^\mu\}, \mu=1,2...P$

Na etapie uczenia metodƒÖ propagacji wstecznej wyznaczane sƒÖ warto≈õci wag $w_{ik}$ oraz ${v_kj}$ ≈ºe
sieƒá taka po nauczeniu mo≈ºe byƒá rozdzielona na dwie czƒô≈õci, lewa to kompresor, a prawa to
dekompresor. Tak zrealizowana kompresja danych jest kompresjƒÖ stratnƒÖ, gdy≈º algorytm uczenia
(propagacja wsteczna) nie daje mo≈ºliwo≈õci zako≈Ñczenia procesu uczenia w sko≈Ñczonym czasie z
b≈Çƒôdem zerowym.

## Zastosowanie sieci neuronowej do wyznaczenia macierzy odwrotnej
Za≈Ç√≥≈ºmy, ≈ºe jest dana macierz kwadratowa $A$ rzƒôdu $n$ ($dim A=(n\times n)$). Nale≈ºy zaprojektowaƒá
sieƒá neuronowƒÖ obliczajƒÖcƒÖ macierz odwrotnƒÖ $B = A^{-1}$.\
$B*A=I$

Za≈Ç√≥≈ºmy, ≈ºe mamy wektor $\vec{x}=[x_1, x_2, ..., x_n]$

$B*A=I/*x$

$BAx=x \Rightarrow BAx-x=0$

$E=\frac{1}{2} ||BAx-x||^2_2$

$||x||_2 = \sqrt{\displaystyle\sum_{i=1}^nx_i^2}$

Zauwa≈ºmy, ≈ºe wektor $\vec{x}$ pe≈Çni podw√≥jnƒÖ rolƒô. Jest wektorem uczƒÖcym, czyli wej≈õciem do
sieci. Pe≈Çni te≈º rolƒô wektora zadanego.

### Zestaw uczƒÖcy
$\{x^\mu,x^\mu\}, \mu=1,2...P$

# Jakie≈õ zadanie
$f_0$ - stan normalny\
$f_1$ - pƒôkniƒôcie\
$f_2$ - uszkodzenie pompy\
$f_3$ - zarastanie

Sygna≈Çy wej≈õciowe:
${P,T,Q}$

Wyj≈õcia:
$f_0,f_1,f_2,f_3$ o warto≈õci $0$ lub $1$\
$1$ - poprawne\
$0$ - niepoprawne

```
graph LR;
P-->net["Sieƒá neuronowa"]
T-->net
Q-->net
net-->f_0
net-->f_1
net-->f_2
net-->f_3
```
## Struktura sieci
3 warstwowa
## Zestaw uczƒÖcy
$
f_0= \begin{Bmatrix}
100& 70& 200 &|1\\
95& 78& 250 &|1\\
\vdots &&&|1\\
93& 71& 210 &|1
\end{Bmatrix}
$

$
f_1=\begin{Bmatrix}
70&40&300&|1 \\
20&45&200&|1\\
\cdots&&&|1
\end{Bmatrix}
$

$f_3=\begin{Bmatrix}
\cdots
\end{Bmatrix}
$

$f_4=\begin{Bmatrix}
\cdots
\end{Bmatrix}
$

# Logika rozmyta i zastosowania
| logika klasyczna                 | logika rozmyta         |
| -                                | -                      |
| Co≈õ albo jest w zbiorze albo nie | Mo≈ºe nale≈ºeƒá czƒô≈õciowo |

## Zastosowanie logiki rozmytej w diagnostyce obiekt√≥w technicznych

```
graph LR;
u-->obiekt
u-->model
obiekt-->|y|o(( ))
model-->|y_u|o
o-->w[r=y-y_u]
```
$r$- sygna≈Ç residuum

$r = \begin{cases}
1 \text{ - gdy nie przekroczony} \\
0 \text{ - gdy przekroczony}
\end{cases}$

### Rozmyta ocena warto≈õci residuum
Ka≈ºdemu residuum $N$ przyporzƒÖdkowaƒá mo≈ºna zmiennƒÖ opisujƒÖcƒÖ wyniki testu. W najgorszym przypadku
zbi√≥r posiada 2 wyniki, pozytywny i negatywny. 

### Regu≈Çy wnioskowania rozmytego
$R_0$ Je≈ºeli $S_1=P$ i $S_j=P$ to wszystko jest OK\
$R_k$ Je≈ºeli $S_1 = N$ i $S_j = P$ to np. Uszkodzenie $f_k$

### Rozmyte wnioskowanie
Analiza zestawu regu≈Ç

#### Rozmyta struktura diagnostyki
```
graph LR;
residuum==>rozm[Blok rozmywania]
rozm==>wn[Blok wnioskowania]
wn==>diagnoza
```
$\{f_1, 0.8\}$ lub $\{f_3, 0.3\}$\
$\{f_0, 1\}$

## Struktura uk≈Çadu sterowania rozmytego

