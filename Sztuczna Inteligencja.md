# Egzamin
25 czerwca (wtorek) 10:00 115 A-2\
4 wrzenia (roda) 10:00 115 A-2

# 娥
# Przykady u偶ycia
## Sprawdzanie parzystoci
### Dane wejciowe
$\vec{u}$ - $n$-elementowy wektor skadajcy si z 0 i 1
### Dane wyjciowe
$1$, jeli liczba jedynek jest nieparzysta,\
$0$ w przeciwnym przypadku
### Struktura sieci
Dwuwarstwowa sie jednokierunkowa z $n$ wej i jednym wyjciu
### Zbi贸r danych uczcych
Udowodniono, 偶e dla wektora o rozmiarze $n$ skadajcego si z $0$ i $1$ wystarczy zbi贸r danych
uczcych o wielkoci $2^n$.

$
{u,z} = \begin{bmatrix}
1 & 0 &0&0&|1\\
1&1&0&0&|0\\
0&1&1&1&|1\\
\vdots&\vdots&\vdots&\vdots&\vdots
\end{bmatrix}
$

## Kompresja
Zadanie kompresji danych polega na zmniejszeniu iloci informacji przechowywanych lub
transmitowanych przy zachowaniu mo偶liwoci jej penego odtworzenia (dekompresji). Mamy kompresj
stratn i bez stratn. Przykadem kompresji bez stratnej mo偶e by drzewo Huffmana
### Dane wejciowe
$n$-elementowy wektor $\vec{u}$
### Dane wyjciowe
$n$-elementowy wektor $\vec{y}$
### Struktura sieci
Tr贸jwarstwowa sie jednokierunkowa

$i=1,2,...,n$ - Indeksy wejciowe\
$j=1,2,...,n$ - Indeksy wyjciowe\
$k=1,2,...,q; q \leq n$ - Indeksy warstwy ukrytej

### Zestaw uczcy
$\{\vec{u}^\mu,\vec{u}^\mu\}, \mu=1,2...P$

Na etapie uczenia metod propagacji wstecznej wyznaczane s wartoci wag $w_{ik}$ oraz ${v_kj}$ 偶e
sie taka po nauczeniu mo偶e by rozdzielona na dwie czci, lewa to kompresor, a prawa to
dekompresor. Tak zrealizowana kompresja danych jest kompresj stratn, gdy偶 algorytm uczenia
(propagacja wsteczna) nie daje mo偶liwoci zakoczenia procesu uczenia w skoczonym czasie z
bdem zerowym.

## Zastosowanie sieci neuronowej do wyznaczenia macierzy odwrotnej
Za贸偶my, 偶e jest dana macierz kwadratowa $A$ rzdu $n$ ($dim A=(n\times n)$). Nale偶y zaprojektowa
sie neuronow obliczajc macierz odwrotn $B = A^{-1}$.\
$B*A=I$

Za贸偶my, 偶e mamy wektor $\vec{x}=[x_1, x_2, ..., x_n]$

$B*A=I/*x$

$BAx=x \Rightarrow BAx-x=0$

$E=\frac{1}{2} ||BAx-x||^2_2$

$||x||_2 = \sqrt{\displaystyle\sum_{i=1}^nx_i^2}$

Zauwa偶my, 偶e wektor $\vec{x}$ peni podw贸jn rol. Jest wektorem uczcym, czyli wejciem do
sieci. Peni te偶 rol wektora zadanego.

### Zestaw uczcy
$\{x^\mu,x^\mu\}, \mu=1,2...P$

# Jakie zadanie
$f_0$ - stan normalny\
$f_1$ - pknicie\
$f_2$ - uszkodzenie pompy\
$f_3$ - zarastanie

Sygnay wejciowe:
${P,T,Q}$

Wyjcia:
$f_0,f_1,f_2,f_3$ o wartoci $0$ lub $1$\
$1$ - poprawne\
$0$ - niepoprawne

```
graph LR;
P-->net["Sie neuronowa"]
T-->net
Q-->net
net-->f_0
net-->f_1
net-->f_2
net-->f_3
```
## Struktura sieci
3 warstwowa
## Zestaw uczcy
$
f_0= \begin{Bmatrix}
100& 70& 200 &|1\\
95& 78& 250 &|1\\
\vdots &&&|1\\
93& 71& 210 &|1
\end{Bmatrix}
$

$
f_1=\begin{Bmatrix}
70&40&300&|1 \\
20&45&200&|1\\
\cdots&&&|1
\end{Bmatrix}
$

$f_3=\begin{Bmatrix}
\cdots
\end{Bmatrix}
$

$f_4=\begin{Bmatrix}
\cdots
\end{Bmatrix}
$

# Logika rozmyta i zastosowania
| logika klasyczna                 | logika rozmyta         |
| -                                | -                      |
| Co albo jest w zbiorze albo nie | Mo偶e nale偶e czciowo |

## Zastosowanie logiki rozmytej w diagnostyce obiekt贸w technicznych

```
graph LR;
u-->obiekt
u-->model
obiekt-->|y|o(( ))
model-->|y_u|o
o-->w[r=y-y_u]
```
$r$- sygna residuum

$r = \begin{cases}
1 \text{ - gdy nie przekroczony} \\
0 \text{ - gdy przekroczony}
\end{cases}$

### Rozmyta ocena wartoci residuum
Ka偶demu residuum $N$ przyporzdkowa mo偶na zmienn opisujc wyniki testu. W najgorszym przypadku
zbi贸r posiada 2 wyniki, pozytywny i negatywny. 

### Reguy wnioskowania rozmytego
$R_0$ Je偶eli $S_1=P$ i $S_j=P$ to wszystko jest OK\
$R_k$ Je偶eli $S_1 = N$ i $S_j = P$ to np. Uszkodzenie $f_k$

### Rozmyte wnioskowanie
Analiza zestawu regu

#### Rozmyta struktura diagnostyki
```
graph LR;
residuum==>rozm[Blok rozmywania]
rozm==>wn[Blok wnioskowania]
wn==>diagnoza
```
$\{f_1, 0.8\}$ lub $\{f_3, 0.3\}$\
$\{f_0, 1\}$

## Struktura ukadu sterowania rozmytego

```
graph LR;
a>"Sygna rzeczywisty"]==>rozm[Blok rozmywania]
rozm==>bp[Blok przetwarzania]
bp==>bw[Blok wyostrzania]
bw==>s>Sterowanie]
```

# Sieci neuronowe
## Nauczanie be nauczyciela (nienadzorowane)

```
graph LR;
u1-->sn
u2-->sn
un-->sn
sn-->y1
sn-->y2
sn-->yn
```
#### Zestaw uczcy
$\{u^\mu\}$

Dotychczas przyjmowano, 偶e cel uczenia by okrelony i nauczanie sieci sprowadzao si do realizacji
tego celu. W nauczaniu bez nauczyciela (nienadzorowanym) kategorie celu s rozwijane przez sie.
Taki spos贸b nauczania rozszerza mo偶liwoci sztucznych sieci neuronowych do zada rozpoznawania
obraz贸w, kiedy klasyfikacje celu s nieznane

## Nauczanie konkursowe - model Rumelharta-Zipsera (model R-Z)
Model R-Z jest sieci klasyfikujc obrazy z jedn warstw neuron贸w przetwarzajcych, kt贸re
konkuruj ka偶dy z ka偶dym. Dla sieci pokazywany jest zbi贸r obraz贸w uczcych, ale nie ma podanego celu
dla tych obraz贸w wyjciowych. Sie taka organizuje obrazy uczce w zbiory klas samodzielnie. Model
R-Z jest prostym schematem i ma mae praktyczne zastosowanie, ale ma zalety pozwalajce na
ilustracje wybranych g贸wnych cech nauczania konkursowego

```
graph LR;
u1-->j1((j1))
u1-->j2
u1-->jn
u2-->j1
u2-->j2((j2))
u2-->jn
un-->j1
un-->j2
un-->jn((jn))
j1-->y1
j2-->y2
jn-->yn

```
### Uwagi
1. Warstwy wejciowe i konkursowe s cakowicie poczone (ka偶dy $i$ty wze jest poczony z ka偶dym
$j$tym wzem)
2. Wagi $W_{i,j}$ posiadajograniczone wartoci $[0,1]$ a ich suma jest r贸wna 1, $0 < W_{ij} < 1,
   \displaystyle\sum_{i=1}^{m}w_{ij} =1$
3. U偶ywa si kwantowania sygna贸w wejciowych i wyjciowych

### Zestaw uczcy
$\{u^\mu\}, \mu = 1,2,...,P$

### Algorytm uczenia
1. Eksperyment: podaj obraz z zestawu uczcego $u^\mu$ i wyznacz potencja membranowy dla
   poszczeg贸lnych wyj. $\phi_j=\displaystyle\sum_{i=1}^m w_{ij} u_i$
2. Analiza: polega na przyjciu algorytmu/pojcia "zwycizca bierze wszystko". W danym przypadku
   zasada ta pozwala na wyznaczenie zwycizcy $\phi_j^z$, kt贸ry posiada najwiksz warto
   potencjau
3. Konsumpcja zwycistwa. Zwycizcy przydzielane jest wyjcie $+1$, a pozostaym $0$. Po okreleniu
   zwycizcy wagi pocze czcych wze zwycizcy s uaktualniane wedug algorytmu:
$\Delta_{ij}^\mu = \alpha(\frac{u_i^\mu}{q}-W_{ij}) = \alpha q^{-1}u_i^\mu - \alpha W_{ij}$\
Gdzie:\
$\alpha$- $(0<\alpha\leq1), \alpha=0.01 - 0.3$\
$q$- liczba wz贸w w warstwie wejciowej, kt贸re posiadaj wartoci $+1$

Aby regua bya speniona, nale偶y sprawdzi warunek\
$\displaystyle\sum_{i=1}^m\Delta W_{ij} = \alpha q^{-1}\displaystyle\sum_i^\mu u_i^\mu -
\alpha\displaystyle\sum_{i-1}^\mu W_{ij} = 0$

## Przykad
### Struktura
3 wejcia $(u_1, u_2, u_3$), 2 wyjcia ($A, B$)
### Obrazy uczce
$u^1 = (1 0 1)P_1$\
$u^2 = (1 0 0)P_2$\
$u^3 = (0 1 0)P_3$\
$u^4 = (0 1 1)P_4$

### Wyniki
W wyniku uzyskujemy:\
$P_1(101) \rightarrow A$\
$P_2(100) \rightarrow A$\
$P_3(010) \rightarrow B$\
$P_3(011) \rightarrow B$

|       | $P_1$ | $P_2$ | $P_3$ | $P_4$ |
| -     | -     | -     | -     | -     |
| $P_1$ |  0    |   1   |   3   |  2    |
| $P_2$ |   1   |  0    |   2   |  3    |
| $P_3$ |   3   |   2   |     0 |  1    |
| $P_4$ |   2   |   3   |    1  |  0    |

## Sieci ze wsp贸zawodnictwem poprzez eliminacje
Dotychczas wze zwycizca w modelu R-Z by okrelany poprzez wyb贸r zewntrzny, to jest wze z
najwikszym potencjaem membranowym $\phi_j$. Alternatywn procedur jest procedura zezwalajca na
eliminacje jednego przez drugiego, czyli w danym przypadku zwycizc jest wze kt贸ry wyeliminowa
wszystkie pozostae, lecz sam pozostaje aktywny.

# Sie Kohonena
Sie konkursowa, kt贸ra organizuje topologiczne odwzorowanie pokazujce rzeczywiste zwizki pomidzy
obrazami prezentowanymi jako wejcia do sieci
## Uwagi
Sygnay wejciowe $u_i$ maj wartoci cige z przedziau $[0-1]$
## Algorytm dziaania sieci
1. Wyznaczy dobre $\vec{u}$ - Eksperyment
2. Obliczy odlego pomidzy obrazem wejciowym a wagami  
   $d_i=\sqrt{\displaystyle\sum_j(u_i-w_{ij})^2}$
3. Wyb贸r zwycizcy dla kt贸rego $d_i$ jest najmniejsze
4. Wybierz ssiedztwo otaczajce wze zwycizcy  
	$n_c$- zbi贸r wz贸w w ssiedztwie zwycizcy
5. Uaktualni wszystkie wagi w ssiedztwie zwycizcy $c(w_{ij})$  
   $\Delta w_{ij}^c = \alpha (u_i - w_{ij}^c)$  
   $w_{ij} = w_{ij}+\Delta w_{ij}^c$  
   $0.2 < \alpha < 0.5$  
Z podanej reguy uczenia wynika, 偶e wagi $w_{ij}$ w otoczeniu zwycizcy s modyfikowane w taki
spos贸b, aby ich wartoci zbli偶ay si do wartoci obrazu wejciowego. Zwycizca $c$, kt贸ry
najbardziej prawdopodobnie mo偶e zwyci偶y wsp贸zawodnictwo powinien obrazy podobne do $u_i$ mie w
pewnej sekwencji
6. Iteracyjne obliczanie wsp贸czynnika $\alpha$

$
\alpha = \alpha_0 (0.2-0.5)\\
\alpha_t = \alpha_0(1-\frac{t}{T})
$  

Gdzie  $t$ - bie偶cy krok, $T$ - og贸lna liczba krok贸w
7. Analiza rozmiaru ssiedztwa $n_c$, na pocztku powinno by du偶e, a w procesie uczenia
   iteracyjnie pomniejszane  

## Algorytm uog贸lniony
1. Wyznaczy zwycizce i jego ssiad贸w
2. Korekta wag, aby poprawi dopasowanie wag do obrazu
3. Korekta (stopniowe zmniejszanie) wsp贸czynnika $\alpha$ i rozmiaru ssiedztwa

## Przykad
Zbi贸r danych z dwoma atrybutami: wiek i doch贸d (znormalizowane). Nale偶y u偶y sieci Kohonena o
rozmiarach $2\times2$, aby odkry ukryte grupy w zbiorze danych

### Dane
$\{\vec{u}^\mu\}=$
| lp. | wiek           | doch贸d          | opis                           |
| -   | -              | -               | -                              |
| 1   | $u_{11} = 0.8$ | $u_{1,2} = 0.8$ | Osoby starsze z du偶ym dochodem |
| 2   | $u_{21} = 0.8$ | $u_{2,2} = 0.1$ | Osoby starsze z maym dochodem |
| 3   | $u_{31} = 0.2$ | $u_{3,2} = 0.9$ | Osoby modsze z du偶ym dochodem |
| 4   | $u_{41} = 0.1$ | $u_{4,2} = 0.1$ | Osoby modsze z maym dochodem |

### Sie
4 neurony wyjciowe $1,2,3,4$, 2 wejciowe $\{\text{wiek}, \text{doch贸d}\}$\
Ze wzgldu na may rozmiar sieci, mo偶na przyj promie ssiedztwa $0$\
$\alpha=0.5$

#### Wagi
$
w_{11}=0.9,
w_{21}=0.8,
w_{12}=0.9,
w_{22}=0.2,\\
w_{13}=0.1,
w_{23}=0.8,
w_{14}=0.1,
w_{24}=0.2
$
### Uczenie
1. Obraz wejciowy nr. 1 $u_1=(0.8, 0.8)$

## Przykad zastosowania sieci Kohonena do budowy klasyfikatora uszkodze rurocigu przesyu medium
### Wejcia:
$p,q,T,V$
### Scenariusze (wyjcia)
- $f_0$ - stan prawidowy
- $f_1$ - pknicie
- $f_2$ - zarastanie
- $f_3$ - inne

### Zbi贸r uczcy
$\{\vec{u}\}, \vec{u} = [p,q,T,v]^T$  
$\{\vec{u^\mu}\}, \mu = 1,2,3...$

$\vec{u}^{f_0}= \begin{bmatrix}
200&150&30&20\\
\vdots & \vdots & \vdots &\vdots
\end{bmatrix}$
